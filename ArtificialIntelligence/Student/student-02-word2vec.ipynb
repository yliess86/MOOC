{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "> **Instructions -**\n",
    "> This notebook contains holes. The holes have to be filled by the student foloowing the instructons provided as functions doc string. A correction will be provided at the end of the lecture. However it is strongly advised to fill the holes by yourself for educational purposes.\n",
    "\n",
    "Text data is different from image data. It is not representend as a tensor. Most of the time text is digitaly represented as a sequence of bytes and encoded using a format such as `ascii` or `utf-8`. One naive way of representing words using vector representation is to keep a vocabulary dictrionnary and assign each word a position. This position can then be used as a one-hot vector, a vector with zeros except at the position of the word index where it is set to one. This the kind of vector we previously built for the target in multi-class classification.\n",
    "\n",
    "While such approach works in some cases, we are limitting the amount of information carried by a word, its meanings, its genre, type, into a single number. To avoid this limitation, one can build a dictionaary of words that are represented by a random vector of a certain size. This is called an embedding. This embedding can then be trained using standard gradient descent by being used in a model.\n",
    "\n",
    "In this chapter, we explore such an embedding called [Word2Vec](https://arxiv.org/abs/1301.3781)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, Linear, Module, PairwiseDistance, ReLU, Sequential\n",
    "from torch.optim import AdamW, Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Iterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harry Potter\n",
    "\n",
    "For the sake of this exploration let us use the Harry Potter Books by J.K. Rowling. The links used for downlaoding the book content are being made available for educational purposes. Please do not use them for other usage.\n",
    "\n",
    "The following class handles the dataset downloading, merging into a single text file, and its reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarryPotterBooks:\n",
    "    base = \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter\"\n",
    "    urls = [\n",
    "        os.path.join(base, \"Book%201%20-%20The%20Philosopher's%20Stone.txt\"),\n",
    "        os.path.join(base, \"Book%202%20-%20The%20Chamber%20of%20Secrets.txt\"),\n",
    "        os.path.join(base, \"Book%203%20-%20The%20Prisoner%20of%20Azkaban.txt\"),\n",
    "        os.path.join(base, \"Book%204%20-%20The%20Goblet%20of%20Fire.txt\"),\n",
    "        os.path.join(base, \"Book%205%20-%20The%20Order%20of%20the%20Phoenix.txt\"),\n",
    "        os.path.join(base, \"Book%206%20-%20The%20Half%20Blood%20Prince.txt\"),\n",
    "        os.path.join(base, \"Book%207%20-%20The%20Deathly%20Hallows.txt\"),\n",
    "    ]\n",
    "\n",
    "    def __init__(self, path: str, download: bool = True) -> None:\n",
    "        self.path = path\n",
    "        if download: self._download()\n",
    "\n",
    "    def _download(self) -> None:\n",
    "        \"\"\"download\n",
    "\n",
    "        Creates a file at self.path location if does not already exisis.\n",
    "        Donwloads every HP books and appends to the file at self.path.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def read_lines(self) -> list[str]:\n",
    "        \"\"\"read_lines\n",
    "\n",
    "        Reads the raw content line by line.\n",
    "        \"\"\"\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can read the first 10 lines of Harry Potter. As we can observe, we may need to clean the content before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_lines = HarryPotterBooks(os.path.join(\".datasets\", f\"hp.txt\"), download=True).read_lines()\n",
    "hp_lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Text is quite a versatile data type. A same word can be used with different meanings, some can be expressed using shortened formats such as abreviations, and more. To make our work easier, let us remove some of this complexity.\n",
    "\n",
    "The following helper class provides methods for removing, cleaning but also joining all the lines into a single welle defined string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarryPotterLinePreprocessor:\n",
    "    @staticmethod\n",
    "    def remove_empty_lines(lines: Iterator[str]) -> Iterator[str]:\n",
    "        \"\"\"remove empty lines\n",
    "        \n",
    "        Removes every empty line from the given set of lines.\n",
    "        Use the filter operation.\n",
    "        \"\"\"\n",
    "        return lines\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_chapter_names(lines: Iterator[str]) -> Iterator[str]:\n",
    "        \"\"\"remove chapter names\n",
    "        \n",
    "        Removes every chapter headers.\n",
    "        Use the filter operation.\n",
    "        \"\"\"\n",
    "        return lines\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_page_declaration(lines: Iterator[str]) -> Iterator[str]:\n",
    "        \"\"\"remove page declaration\n",
    "        \n",
    "        Removes every page footer.\n",
    "        Use the filter operation.\n",
    "        \"\"\"\n",
    "        return lines\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_abbreviations(lines: Iterator[str]) -> Iterator[str]:\n",
    "        \"\"\"replace abbreviations\n",
    "        \n",
    "        Replace abbreviations by their full version (Dr. -> Doctor).\n",
    "        Use the map operation.\n",
    "        \"\"\"\n",
    "        return lines\n",
    "\n",
    "    def remove_end_of_line(lines: Iterator[str]) -> Iterator[str]:\n",
    "        \"\"\"remove end of line\n",
    "        \n",
    "        Remove all end of line symbols.\n",
    "        Use the map operation.\n",
    "        \"\"\"\n",
    "        return lines\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(lines: Iterator[str], join: bool = True) -> Iterator[str] | str:\n",
    "        lines = HarryPotterLinePreprocessor.remove_empty_lines(lines)\n",
    "        lines = HarryPotterLinePreprocessor.remove_chapter_names(lines)\n",
    "        lines = HarryPotterLinePreprocessor.remove_page_declaration(lines)\n",
    "        lines = HarryPotterLinePreprocessor.replace_abbreviations(lines)\n",
    "        lines = HarryPotterLinePreprocessor.remove_end_of_line(lines)\n",
    "        return \" \".join(lines) if join else list(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can observe the first thousand characters of the joined and cleaned string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_raw = HarryPotterLinePreprocessor.preprocess(hp_lines, join=True)\n",
    "hp_raw[:1_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "In order to build a dictionnary for our embedding, we need to split the text in single word units called tokens. The text is first split into sentences. The reason is explained later in the document.\n",
    "\n",
    "THe following processor class provides functions to split the text into sentences, then into tokens, but also removes the english punctuation symbols, makes each token lower case to avoid repetition, and keeps the sentences that have a minimum given length (number of tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarryPotterProcessor:\n",
    "    @staticmethod\n",
    "    def extract_sentences(raw: str) -> list[str]:\n",
    "        \"\"\"extract sentences\n",
    "        \n",
    "        Split raw text by sentences.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_sentences_tokens(sentences: list[str], min_seq: int) -> list[list[str]]:\n",
    "        \"\"\"extract sentences tokens\n",
    "        \n",
    "        For each sentence, split the sentence by tokens.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    @staticmethod\n",
    "    def to_lower(sentences_tokens: list[list[str]]) -> list[list[str]]:\n",
    "        \"\"\"to lower\n",
    "        \n",
    "        Put every token in its lower case version.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def lemmatize(sentences_tokens: list[list[str]]) -> list[list[str]]:\n",
    "        \"\"\"lemmatize\n",
    "        \n",
    "        Keep only the root of the tokens using a lemmatizer. (eating -> eat)\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def remove_stop_words(sentences_tokens: list[list[str]]) -> list[list[str]]:\n",
    "        \"\"\"remove stop words\n",
    "        \n",
    "        Remove every english stop words from the tokens.\n",
    "        \"\"\"\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.update([\n",
    "            \".\", \",\", \":\", \";\", \"?\", \"!\",\n",
    "            \"(\", \")\", \"#\", \"—\", \"-\", \"_\", \"--\", \"...\",\n",
    "            \"“\", \"”\", \"\\\"\", \"'\", \"`\", \"~\", \"’\", \"|\",\n",
    "        ])\n",
    "        return []\n",
    "\n",
    "    @staticmethod\n",
    "    def extract(raw: str, min_seq: int = 9) -> list[list[str]]:\n",
    "        sentences = HarryPotterProcessor.extract_sentences(raw)\n",
    "        sentences_tokens = HarryPotterProcessor.extract_sentences_tokens(sentences, min_seq)\n",
    "        sentences_tokens = HarryPotterProcessor.to_lower(sentences_tokens)\n",
    "        sentences_tokens = HarryPotterProcessor.remove_stop_words(sentences_tokens)\n",
    "        sentences_tokens = HarryPotterProcessor.lemmatize(sentences_tokens)\n",
    "        return sentences_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "The vocabulary is an augmented dictionary containing the index of every token in a given token list. It provides methods to transform a token, a string, into its index in the dictionary, and the inverse operation to transform an index to its corresponding token. Here the vocabulary only keeps the token that appears at least some given number of time in the text. It let us avoid creating an enormous dataset that woul be impracticable later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens: list[str], min_freq: int) -> None:\n",
    "        self.t2f = OrderedDict(Counter(tokens).most_common())\n",
    "        self._stoi = {s: i for i, (s, count) in enumerate([(\"<unk>\", min_freq)] + list(self.t2f.items())) if count >= min_freq}\n",
    "        self._itos = {i: s for s, i in self._stoi.items()}\n",
    "\n",
    "    def __len__(self) -> int: return len(self._stoi)\n",
    "    def stoi(self, s: str) -> int: return self._stoi[s] if s in self._stoi else 0\n",
    "    def itos(self, i: int) -> int: return self._itos[i] if i in self._itos else \"<unk>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continous Bag of Words (CBOW)\n",
    "\n",
    "A word can be explained by its surrounding, the context. Gvien a sentence, if one word is removes, it is likely you are able to find the missing one. This is due to the word surrounding the one masked. It gives us information that allows to narrow down the posibilities to a few. This is called a prior. Continuous Bag of Word (CBOW) is an embedding training method making use of this phenomena. Instead of encoding the word embedding into a latent variable and decoding it to find the input embedding back, it uses the context word as inputs that are aggregated, encoded into a latent variable that is then decoded into the target word.\n",
    "\n",
    "The Harry Potter CBOW Dataset class is reponsible for building a dataset of context annd target word pairs. It first builds a vocabulary dictionary out of the Harry Potter Books content and uses it to output tokens as interger indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarryPotterCBOWDataset(Dataset):\n",
    "    def __init__(self, raw: str, window_size: int, min_word_freq: int) -> None:\n",
    "        super().__init__()\n",
    "        assert window_size // 2 != 0\n",
    "        self.raw = raw\n",
    "        self.window_size = window_size\n",
    "        self.min_word_freq = min_word_freq\n",
    "        self.vocab: Vocab() = None\n",
    "        self.pairs: list[tuple[list[str], str]] = []\n",
    "        self._build()\n",
    "\n",
    "    def _build(self) -> None:\n",
    "        \"\"\"build\n",
    "\n",
    "        Extract the sentences tokens, build the vocabulary and the context / target\n",
    "        word pairs. The pairs are obtained using a sliding window with a hop size of\n",
    "        1 for sentences that have a minimum length at least equal to the window size + 1.\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"len\n",
    "        \n",
    "        Returns the dataset length.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[Tensor, int]:\n",
    "        \"\"\"getitem\n",
    "        \n",
    "        Retrieve the context / target token pairs.\n",
    "        Return the context as a Tensor of indices from the vocabulary corresponding to\n",
    "        the context tokens and a single index corresponding to the target word index.\n",
    "        \"\"\"\n",
    "        return None, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we display the Dataset length, the vocabulary size, and the shape of the context to verify that it feats with respect to our arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_cbow = HarryPotterCBOWDataset(hp_raw, window_size=9, min_word_freq=10)\n",
    "ctx, word = hp_cbow[0]\n",
    "print(\"Dataset Length:\", len(hp_cbow))\n",
    "print(\"Vocab   Length:\", len(hp_cbow.vocab))\n",
    "print(\"Context Size  :\", tuple(ctx.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Word2Vec\n",
    "\n",
    " Word2Vec is a simple AutoEncoder model. It is composed of an embedding that is responsible for storing the embedding vector for each and every word of the dictionary, and a multi layer perceptron that is responsible for the decoding portion of the network. The model takes a context vector as input $(n_{\\text{context}}, \\text{embedding}_{\\text{size}})$, and outputs a probability vector that is optimized for outputing the one hot representation of the target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        emb_dim: int,\n",
    "        h_dim: int,\n",
    "        context_size: int,\n",
    "        emb_max_norm: float = 1.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, embedding_dim=emb_dim, max_norm=emb_max_norm)\n",
    "        self.classifier = Sequential(\n",
    "            Linear(emb_dim * context_size, h_dim), ReLU(),\n",
    "            Linear(h_dim, vocab_size, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"forward\n",
    "        \n",
    "        Forward pass for the Word2Vec model in CBOW mode.\n",
    "            ctx_1 \\\n",
    "            ctx_2 -> z -> tgt\n",
    "            ctx_3 /\n",
    "        \"\"\"\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class History:\n",
    "    def __init__(self) -> None:\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        assert len(self.loss) == len(self.acc)\n",
    "        return len(self.loss)\n",
    "\n",
    "    def plot(self) -> None:\n",
    "        fig = plt.figure(figsize=(2 * 4, 4), dpi=400)\n",
    "        ax = fig.add_subplot(2, 1, 1)\n",
    "        ax.plot(list(range(len(self))), self.loss, label=\"train\", color=\"r\")\n",
    "        ax.set_title(\"Loss History\")\n",
    "        ax.set_xlabel(\"epoch\")\n",
    "        ax.set_ylabel(\"loss\")\n",
    "        ax.grid(linestyle=\"--\", linewidth=0.5)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"bottom\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        ax = fig.add_subplot(2, 1, 2)\n",
    "        ax.plot(list(range(len(self))), self.acc, label=\"train\", color=\"b\")\n",
    "        ax.set_title(\"Accuracy History\")\n",
    "        ax.set_xlabel(\"epoch\")\n",
    "        ax.set_ylabel(\"accuracy in %\")\n",
    "        ax.set_ylim(0.0, 100.0)\n",
    "        ax.grid(linestyle=\"--\", linewidth=0.5)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"bottom\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        fig.canvas.draw()\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model: Module, optim: Optimizer, loader: DataLoader, epochs: int, device: str) -> History:\n",
    "    \"\"\"fit\n",
    "    \n",
    "    Fit the given embedding model to the given dataset.\n",
    "    Should use tqdm to display a progress bar and fill the history for each\n",
    "    epoch (for the average loss and accuracy).\n",
    "    \"\"\"\n",
    "    history = History()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "epochs = 50\n",
    "batch_size = 512\n",
    "lr = 1e-3\n",
    "\n",
    "vocab_size = len(hp_cbow.vocab)\n",
    "emb_dim = 512\n",
    "h_dim = 128\n",
    "context_size = hp_cbow.window_size - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader, Model, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(hp_cbow, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)\n",
    "model = Word2Vec(vocab_size, emb_dim, h_dim, context_size).to(device)\n",
    "optim = AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, optim, loader, epochs, device).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Visualization\n",
    "\n",
    "Here we attempt to visualize the resulting embedding space. As the embedding dim is often bigger thant 3 dimension it is hard to visualize. In this type of scenario data scientists use a set of tools referred to as dimensionality reduction techniques. One of such as method is [t-SNE](https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf).\n",
    "\n",
    "In the following cell, we apply the t-SNE method to every token of the vocabulary and reduce their embedding dimension to 2. This process will allow us to project the embeddings into a 2 dimensional scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = model.embedding.to(\"cpu\")\n",
    "\n",
    "vocab_emb = torch.empty((len(hp_cbow.vocab), emb_dim), dtype=torch.float32)\n",
    "with torch.inference_mode():\n",
    "    for word_id in range(len(hp_cbow.vocab)):\n",
    "        # Fill the vocabulary embedding list for every word in the volcabulary\n",
    "        ...\n",
    "\n",
    "reduced_vocab_emb = TSNE(n_components=2).fit_transform(vocab_emb.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8), dpi=400)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(x=reduced_vocab_emb[1:, 0], y=reduced_vocab_emb[1:, 1], color=\"b\", marker=\"+\")\n",
    "for i, word in hp_cbow.vocab._itos.items():\n",
    "    if word == \"<unk>\": continue\n",
    "    ax.annotate(word, reduced_vocab_emb[i])\n",
    "ax.set_title(\"Harry Potter Word Embedding\")\n",
    "ax.set_xlabel(\"t-sne axis 1\")\n",
    "ax.set_ylabel(\"t-sne axis 2\")\n",
    "ax.grid(linestyle=\"--\", linewidth=0.5)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "fig.canvas.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closest Words\n",
    "\n",
    "The trained embedding makes word with similar contexts close to each other and pushes others far away. As such we can mesure the closest relationship between every word of the dictionnary by mesure their distance. Many distances can be used such as the euclidean distance (used here), the cosine distance, and more. They all have downsides and advantages that will not be discussed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_words(ref_word: str, top_k: int = 10) -> None:\n",
    "    ref_idx = torch.tensor(hp_cbow.vocab.stoi(ref_word), dtype=torch.long)\n",
    "    ref_embedding = model.embedding(ref_idx)\n",
    "\n",
    "    word_distances = []\n",
    "    pdist = PairwiseDistance()\n",
    "    for word in hp_cbow.vocab._stoi.keys():\n",
    "        if word in [ref_word, \"<unk>\"]: continue\n",
    "        idx = torch.tensor(hp_cbow.vocab.stoi(word), dtype=torch.long)\n",
    "        embedding = model.embedding(idx)\n",
    "        word_distances.append((word, pdist(ref_embedding, embedding).item()))\n",
    "    word_distances.sort(key=lambda wd: wd[1])\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Closest words for {ref_word}:\")\n",
    "    for i, (word, dist) in enumerate(word_distances[:top_k]):\n",
    "        print(f\"{i + 1:>2} - {word:<12} | {dist:.2e}\")\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_words(\"harry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_words(\"wizzard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_words(\"wand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_words(\"griffondor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_words(\"slytherin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_words(\"death\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_words(\"car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Arithmetic\n",
    "\n",
    "By representing words as vectors, it naturally benefits from vector space operations. Thus, we can apply arithmetics operations to word and combine them in a meaningful manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plus_minus(ref_word: str, plus_word: str, minus_word: str, top_k: int = 10) -> None:\n",
    "    ref_idx = torch.tensor(hp_cbow.vocab.stoi(ref_word), dtype=torch.long)\n",
    "    ref_embedding = model.embedding(ref_idx)\n",
    "\n",
    "    plus_idx = torch.tensor(hp_cbow.vocab.stoi(plus_word), dtype=torch.long)\n",
    "    plus_embedding = model.embedding(plus_idx)\n",
    "\n",
    "    minus_idx = torch.tensor(hp_cbow.vocab.stoi(minus_word), dtype=torch.long)\n",
    "    minus_embedding = model.embedding(minus_idx)\n",
    "\n",
    "    op_embedding = ref_embedding + plus_embedding - minus_embedding\n",
    "\n",
    "    word_distances = []\n",
    "    pdist = PairwiseDistance()\n",
    "    for word in hp_cbow.vocab._stoi.keys():\n",
    "        if word in [ref_word, plus_word, minus_word, \"<unk>\"]: continue\n",
    "        idx = torch.tensor(hp_cbow.vocab.stoi(word), dtype=torch.long)\n",
    "        embedding = model.embedding(idx)\n",
    "        word_distances.append((word, pdist(op_embedding, embedding).item()))\n",
    "    word_distances.sort(key=lambda wd: wd[1])\n",
    "\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Closest words for {ref_word} + {plus_word} - {minus_word}:\")\n",
    "    for i, (word, dist) in enumerate(word_distances[:top_k]):\n",
    "        print(f\"{i + 1:>2} - {word:<12} | {dist:.2e}\")\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus_minus(\"harry\", \"ginny\", \"hermione\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus_minus(\"james\", \"lily\", \"child\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8), dpi=400)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "friends   = [\"harry\", \"ron\", \"hermione\", \"hagrid\"]\n",
    "voldemort = [\"voldemort\", \"tom\", \"riddle\", \"lord\"]\n",
    "magic     = [\"wand\", \"expelliarmus\", \"avada\", \"patronum\", \"leviosa\"]\n",
    "houses    = [\"gryffindor\", \"ravenclaw\", \"hufflepuff\", \"slytherin\"]\n",
    "quidditch = [\"quidditch\", \"quaffle\", \"chase\", \"broomstick\", \"gold\", \"snitch\"]\n",
    "\n",
    "c = []\n",
    "for i, word in hp_cbow.vocab._itos.items():\n",
    "    found = False\n",
    "    if word == \"<unk>\"  : continue\n",
    "    if word in friends  : found = True; c.append((1, 0, 0, 1))\n",
    "    if word in voldemort: found = True; c.append((0, 1, 0, 1))\n",
    "    if word in magic    : found = True; c.append((0, 0, 1, 1))\n",
    "    if word in houses   : found = True; c.append((1, 0, 1, 1))\n",
    "    if word in quidditch: found = True; c.append((1, 1, 0, 1))\n",
    "    if found: ax.annotate(word, reduced_vocab_emb[i])\n",
    "    else: c.append((0, 0, 0, 0.1))\n",
    "\n",
    "ax.scatter(x=reduced_vocab_emb[1:, 0], y=reduced_vocab_emb[1:, 1], c=c, marker=\"+\")\n",
    "ax.set_title(\"Harry Potter Colored Word Embedding\")\n",
    "ax.set_xlabel(\"t-sne axis 1\")\n",
    "ax.set_ylabel(\"t-sne axis 2\")\n",
    "ax.grid(linestyle=\"--\", linewidth=0.5)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "fig.canvas.draw()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
